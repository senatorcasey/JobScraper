{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from dataclasses import dataclass\n",
    "from pympler import asizeof\n",
    "import copy\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "from fake_useragent import UserAgent\n",
    "import psutil\n",
    "import time\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the logging\n",
    "# level = DEBUG < INFO < ERROR < WARNING < CRITICAL\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger('\\tJobScraper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some defaults to use everywhere\n",
    "defMaxAge = 1\n",
    "defMinFuzzScore = 60\n",
    "defRegex = (\" \", \"%20\")\n",
    "\n",
    "searchTerms = ['salesforce administrator', 'junior salesforce', 'jr salesforce', 'entry level salesforce']\n",
    "\n",
    "outfile = 'slots.csv'\n",
    "torWaitTime = random.randint(20,30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewCircuit():\n",
    "    sleeptime = random.randint(20,120)\n",
    "    logger.info(f\"\\tgetJobs:\\tsleeping {sleeptime}s for a new Tor circuit\")\n",
    "    time.sleep(sleeptime)\n",
    "    with Controller.from_port(port = 9051) as c:\n",
    "        c.authenticate()\n",
    "        c.signal(Signal.NEWNYM)\n",
    "        newIP = requests.get(\"https://ident.me\", proxies=proxies).text\n",
    "        logger.info(f\"\\tgetJobs:\\tnew IP: {newIP}\")\n",
    "\n",
    "# creates a proxies dictionary if Tor is running\n",
    "def getProxies():\n",
    "    #Iterate over the all the running process\n",
    "    for proc in psutil.process_iter():\n",
    "        try:\n",
    "            # Check if process name contains the given name string.\n",
    "            if 'tor' in proc.name().lower():\n",
    "                return { 'http': 'socks5://127.0.0.1:9050', \n",
    "                        'https': 'socks5://127.0.0.1:9050' }\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "proxies=getProxies()\n",
    "if proxies:\n",
    "    logger.info(\"\\tTor running, using proxies\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a config object\n",
    "@dataclass\n",
    "class ScraperConfig:\n",
    "    __slots__ = ['searchTerm', 'maxAge', 'searchURL', \n",
    "                 'minFuzzScore', 'regex', 'baseURL', 'headers', \n",
    "                 'jobCardSelector', 'nameSelector', 'hrefSelector', 'dateSelector', 'companySelector', \n",
    "                 'locationSelector', 'detailSelector']\n",
    "    searchTerm: str\n",
    "    maxAge: int\n",
    "    searchURL: str\n",
    "    minFuzzScore: int\n",
    "    regex: tuple\n",
    "    baseURL: str\n",
    "    headers: dict\n",
    "    jobCardSelector: str\n",
    "    nameSelector: str\n",
    "    hrefSelector: str\n",
    "    dateSelector: str\n",
    "    companySelector: str\n",
    "    locationSelector: str\n",
    "    detailSelector: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a job object\n",
    "@dataclass\n",
    "class Job:\n",
    "    __slots__ = ['name', 'url', 'description', 'company', 'location', \n",
    "                 'posted', 'closes', 'stage', 'fuzzyWuzzy', 'config', 'daysOld']\n",
    "    name: str\n",
    "    url: str\n",
    "    description: str\n",
    "    company: str\n",
    "    location: str\n",
    "    posted: datetime.date\n",
    "    closes: datetime.date\n",
    "    stage: str\n",
    "    fuzzyWuzzy: int\n",
    "    config: ScraperConfig\n",
    "    daysOld: int\n",
    "    \n",
    "    # mostly just for quick debugging\n",
    "    def __str__(self):\n",
    "        return f'Name: {self.name}\\n URL: {self.url}'\n",
    "    \n",
    "    \n",
    "    # get the remaining details from the job's url field\n",
    "    def retrieveDetails(self):\n",
    "        # if Tor is running, get a new IP\n",
    "        if proxies: getNewCircuit()\n",
    "            \n",
    "        # get the actual job page\n",
    "        logger.debug(f\"\\tretrieveDetails:\\tgetting {self.url}\\nusing headers {self.config.headers}\")\n",
    "        page = requests.get(self.url, headers={'User-Agent':UserAgent().random}, proxies=proxies)\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        \n",
    "        # parse out the remaining details\n",
    "        logger.debug(f\"\\tretrieveDetails:\\tlooking for '{self.config.locationSelector}' and '{self.config.detailSelector}'\")\n",
    "        self.location = soup.select_one(self.config.locationSelector).text.strip()\n",
    "        self.description = html2text.html2text(soup.select_one(self.config.detailSelector).prettify())\n",
    "        \n",
    "        #logger.debug(f\"\\tretrieveDetails:\\n{self.name}\\n{self.location}\\n{self.description}\\n\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # returns a list for a CSV file\n",
    "    def getDataloader(self):\n",
    "        return [self.name,\n",
    "               self.url,\n",
    "               self.description,\n",
    "               self.posted.strftime(\"%m/%d/%Y\"),\n",
    "               self.closes.strftime(\"%m/%d/%Y\"),\n",
    "               self.stage]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the job scrapers\n",
    "@dataclass\n",
    "class JobScraper(object):\n",
    "    __slots__ = ['config']\n",
    "    config: ScraperConfig\n",
    "        \n",
    "    def getJobs(self):\n",
    "        jobs = []\n",
    "        searchTerm = \"\"\n",
    "        \n",
    "        # apply the regex to the search term(s) if necessary\n",
    "        try:\n",
    "            searchTerm = re.sub(self.config.regex[0], self.config.regex[1], self.config.searchTerm)\n",
    "\n",
    "        except:\n",
    "            searchTerm = self.config.searchTerm\n",
    "            \n",
    "        # if Tor is running, get a new IP\n",
    "        if proxies: getNewCircuit()\n",
    "                \n",
    "        # get the page\n",
    "        try:\n",
    "            searchURL = self.config.searchURL.format(searchTerm, self.config.maxAge)\n",
    "            logger.debug(f\"\\tgetJobs:\\t {searchURL}\")\n",
    "            page = requests.get(searchURL, headers={'User-Agent':UserAgent().random}, proxies=proxies)\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            cards = soup.select(self.config.jobCardSelector)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"\\tgetJobs:\\t unable to retrieve jobs from {searchURL}\\n{e}\")\n",
    "            return jobs\n",
    "        \n",
    "        # get all the jobs\n",
    "        for card in cards:\n",
    "            try:\n",
    "                name = card.select_one(self.config.nameSelector).text.strip()\n",
    "                fuzzy = fuzz.ratio(self.config.searchTerm, name)\n",
    "                href = self.config.baseURL + card.select_one(self.config.hrefSelector)['href']\n",
    "                postedRelative = card.select_one(self.config.dateSelector).text.strip().upper()\n",
    "                company = card.select_one(self.config.companySelector).text.strip()\n",
    "\n",
    "                # skip it if the fuzz score is too low\n",
    "                if (fuzzy < self.config.minFuzzScore):\n",
    "                    logger.info(f\"\\tgetJobs:\\tignoring [{fuzzy}]\\t{name}\")\n",
    "                    continue\n",
    "\n",
    "                logger.debug(f\"\\tgetJobs:\\ttrying to parse {postedRelative}\")\n",
    "                daysOld = 9999\n",
    "                try:\n",
    "                    if (postedRelative in ['TODAY', 'JUST POSTED', 'POSTED TODAY', '24H']):\n",
    "                        daysOld = 0\n",
    "                    else:\n",
    "                        i = re.findall(r\"^\\d+\", postedRelative)\n",
    "                        daysOld = int(i[0])\n",
    "                except:\n",
    "                    logger.error(f\"\\tgetJobs:\\tunable to parse {postedRelative}\")\n",
    "\n",
    "                # skip it if it's older than the max age\n",
    "                if (daysOld > self.config.maxAge):\n",
    "                    logger.info(f\"\\tgetJobs:\\tignoring {daysOld} days old - {name}\")\n",
    "                    continue\n",
    "\n",
    "                j = Job(name = name, \n",
    "                        url = href, \n",
    "                        description = None, \n",
    "                        company = company, \n",
    "                        location = None,\n",
    "                        posted = date.today() - timedelta(days=daysOld), \n",
    "                        closes = date.today() - timedelta(days=daysOld) + timedelta(days=14), \n",
    "                        stage = \"new\", \n",
    "                        fuzzyWuzzy = fuzzy, \n",
    "                        config = self.config, \n",
    "                        daysOld = daysOld)\n",
    "                jobs.append(j)\n",
    "            except Exception as e:\n",
    "                logger.info(f\"\\tgetJobs:\\tException: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a configuration for each website you want to search\n",
    "configs = []\n",
    "\n",
    "# indeed\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic indeed config\",\n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.indeed.com/jobs?q={0}&l=remote&fromage={1}\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = defRegex,\n",
    "    baseURL = \"https://www.indeed.com\",\n",
    "    headers = None,\n",
    "    \n",
    "    jobCardSelector = \".result\",\n",
    "    nameSelector = \".jobtitle\",\n",
    "    hrefSelector = \".jobtitle\",\n",
    "    dateSelector = \".date\",\n",
    "    companySelector = \"span.company\",\n",
    "    \n",
    "    locationSelector = \".icl-u-xs-mt--xs > div > div:nth-of-type(3)\",\n",
    "    detailSelector = \"#jobDescriptionText\"\n",
    "))\n",
    "\n",
    "# career builder\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic career builder config\", \n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.careerbuilder.com/jobs?posted={1}&pay=&cat1=&radius=&emp=&cb_apply=false&keywords={0}&location=&cb_workhome=true\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = defRegex,\n",
    "    baseURL = \"https://www.careerbuilder.com\",\n",
    "    headers = None,\n",
    "    \n",
    "    jobCardSelector = \".data-results-content-parent\",\n",
    "    nameSelector = \".data-results-title\",\n",
    "    hrefSelector = \"a.job-listing-item\",\n",
    "    dateSelector = \".data-results-publish-time\",\n",
    "    companySelector = \".data-details > span:nth-of-type(1)\",\n",
    "    \n",
    "    locationSelector = \".data-details > span:nth-of-type(3)\",\n",
    "    detailSelector = \"#jdp_description > div:nth-of-type(1) > div:nth-of-type(1)\"\n",
    "))\n",
    "\n",
    "# monster\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic monster config\",\n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.monster.com/jobs/search/?q={0}&tm={1}\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = defRegex,\n",
    "    baseURL = \"\",\n",
    "    headers = None,\n",
    "    \n",
    "    jobCardSelector = \"div.flex-row\",\n",
    "    nameSelector = \".title\",\n",
    "    hrefSelector = \".title > a\",\n",
    "    dateSelector = \"time\",\n",
    "    companySelector = \"div.company\",\n",
    "    \n",
    "    locationSelector = \"div.location\",\n",
    "    detailSelector = \"div.job-description\"\n",
    "))\n",
    "\n",
    "# glass door\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic glass door config\",\n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.glassdoor.com/Job/{0}-jobs-SRCH_KO0,24.htm?fromAge={1}&remoteWorkType=1\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = (\" \", \"-\"),\n",
    "    baseURL = \"https://www.glassdoor.com\",\n",
    "    headers = None,\n",
    "    \n",
    "    jobCardSelector = \"li.jl\",\n",
    "    nameSelector = \"a.jobTitle > span\",\n",
    "    hrefSelector = \"a.jobTitle\",\n",
    "    dateSelector = \"div[data-test='job-age']\",\n",
    "    companySelector = \"a.jobLink > span\",\n",
    "    \n",
    "    locationSelector = \"div.flex-column > div > div:nth-of-type(3)\",\n",
    "    detailSelector = \"div.desc\"\n",
    "))\n",
    "\n",
    "logger.debug(f\"\\tsize of configs as objects = {asizeof.asizesof(configs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actually do the job scraping now!\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    starttime = datetime.now()\n",
    "    allJobs = []\n",
    "    scrapers = []\n",
    "    \n",
    "    # create a job scraper for every config\n",
    "    [scrapers.append(JobScraper(c)) for c in configs]\n",
    "    logger.debug(f\"\\tmain:\\t{len(scrapers)} scrapers created\")\n",
    "    \n",
    "    # repeat the searches for each of the terms in the list\n",
    "    for st in searchTerms:\n",
    "        logger.info(f\"\\tmain:\\tsetting the searchTerm to {st}\")\n",
    "        for s in scrapers:\n",
    "            s.config.searchTerm = st\n",
    "\n",
    "        # Insert tasks into the queue and let them run\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for s in scrapers:\n",
    "                futures.append(executor.submit(s.getJobs))\n",
    "\n",
    "            # as threads complete, add their jobs to the allJobs list\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                logger.info(f\"\\tmain:\\tadding {len(future.result())} jobs to allJobs\")\n",
    "                for r in future.result():\n",
    "                    logger.debug(f\"\\tmain:\\tadding {r.name} to allJobs\")\n",
    "                    allJobs.append(r)\n",
    "    \n",
    "    # Create another threadpool for all the details\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        logger.info(f\"\\tmain:\\tadding {len(allJobs)} threads to the executor\")\n",
    "        for j in allJobs:\n",
    "            logger.debug(f\"\\tmain:\\tadding thread for {j.name}\")\n",
    "            futures.append(executor.submit(j.retrieveDetails))\n",
    "    \n",
    "    endtime = datetime.now()\n",
    "    logger.info(f\"\\tmain:\\ttime to run: {endtime - starttime}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    newFile = not(os.path.isfile(outfile) and os.path.getsize(outfile) > 0)\n",
    "    \n",
    "    # create our csv file for dataloader\n",
    "    with open(outfile, mode='a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        if newFile:\n",
    "            writer.writerow(['Name','OriginalURL','Description','PostingDate','CloseDate','Stage'])\n",
    "\n",
    "        # then write each collection of jobs to the file\n",
    "        for j in allJobs:\n",
    "            writer.writerow(j.getDataloader())\n",
    "\n",
    "    logger.info(f\"\\tmain:\\t{outfile} created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
