{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from dataclasses import dataclass\n",
    "from pympler import asizeof\n",
    "import copy\n",
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "from fake_useragent import UserAgent\n",
    "import psutil\n",
    "import time\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the logging\n",
    "# level = DEBUG < INFO < ERROR < WARNING < CRITICAL\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger('\\tJobScraper')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup some defaults to use everywhere\n",
    "defMaxAge = 1\n",
    "defMinFuzzScore = 60\n",
    "defRegex = (\" \", \"%20\")\n",
    "\n",
    "searchTerms = ['salesforce administrator', 'junior salesforce', 'jr salesforce', 'entry level salesforce']\n",
    "\n",
    "outfile = 'slots.csv'\n",
    "#torPorts = None\n",
    "#torPorts = ['9050']\n",
    "torPorts = ['9050', '9150', '9250', '9350']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a config object\n",
    "@dataclass\n",
    "class ScraperConfig:\n",
    "    __slots__ = ['searchTerm', 'maxAge', 'searchURL', \n",
    "                 'minFuzzScore', 'regex', 'baseURL', 'headers', 'proxies', \n",
    "                 'jobCardSelector', 'nameSelector', 'hrefSelector', 'dateSelector', 'companySelector', \n",
    "                 'locationSelector', 'detailSelector']\n",
    "    searchTerm: str\n",
    "    maxAge: int\n",
    "    searchURL: str\n",
    "    minFuzzScore: int\n",
    "    regex: tuple\n",
    "    baseURL: str\n",
    "    headers: dict\n",
    "    proxies: dict\n",
    "    jobCardSelector: str\n",
    "    nameSelector: str\n",
    "    hrefSelector: str\n",
    "    dateSelector: str\n",
    "    companySelector: str\n",
    "    locationSelector: str\n",
    "    detailSelector: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a job object\n",
    "@dataclass\n",
    "class Job:\n",
    "    __slots__ = ['name', 'url', 'description', 'company', 'location', \n",
    "                 'posted', 'closes', 'stage', 'fuzzyWuzzy', 'config', 'daysOld']\n",
    "    name: str\n",
    "    url: str\n",
    "    description: str\n",
    "    company: str\n",
    "    location: str\n",
    "    posted: datetime.date\n",
    "    closes: datetime.date\n",
    "    stage: str\n",
    "    fuzzyWuzzy: int\n",
    "    config: ScraperConfig\n",
    "    daysOld: int\n",
    "    \n",
    "    # mostly just for quick debugging\n",
    "    def __str__(self):\n",
    "        return f'Name: {self.name}\\n URL: {self.url}'\n",
    "    \n",
    "    \n",
    "    # get the remaining details from the job's url field\n",
    "    def retrieveDetails(self):\n",
    "        # slow the thread down by waiting a while\n",
    "        sleeptime = random.randint(20,120)\n",
    "        logger.info(f\"\\tretrieveDetails:\\tsleeping {sleeptime}s before getting details\")\n",
    "        time.sleep(sleeptime)\n",
    "            \n",
    "        # get the actual job page\n",
    "        logger.debug(f\"\\tretrieveDetails:\\tgetting {self.url}\\nusing headers {self.config.headers}\")\n",
    "        page = requests.get(self.url, headers=self.config.headers, proxies=self.config.proxies)\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "        \n",
    "        # parse out the remaining details\n",
    "        logger.debug(f\"\\tretrieveDetails:\\tlooking for '{self.config.locationSelector}' and '{self.config.detailSelector}'\")\n",
    "        self.location = soup.select_one(self.config.locationSelector).text.strip()\n",
    "        self.description = html2text.html2text(soup.select_one(self.config.detailSelector).prettify())\n",
    "        \n",
    "        #logger.debug(f\"\\tretrieveDetails:\\n{self.name}\\n{self.location}\\n{self.description}\\n\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # returns a list for a CSV file\n",
    "    def getDataloader(self):\n",
    "        return [self.name,\n",
    "               self.url,\n",
    "               self.description,\n",
    "               self.posted.strftime(\"%m/%d/%Y\"),\n",
    "               self.closes.strftime(\"%m/%d/%Y\"),\n",
    "               self.stage]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the job scrapers\n",
    "@dataclass\n",
    "class JobScraper(object):\n",
    "    __slots__ = ['config']\n",
    "    config: ScraperConfig\n",
    "        \n",
    "    def getJobs(self):\n",
    "        jobs = []\n",
    "        searchTerm = \"\"\n",
    "        \n",
    "        # apply the regex to the search term(s) if necessary\n",
    "        try:\n",
    "            searchTerm = re.sub(self.config.regex[0], self.config.regex[1], self.config.searchTerm)\n",
    "\n",
    "        except:\n",
    "            searchTerm = self.config.searchTerm\n",
    "            \n",
    "        # get the page\n",
    "        try:\n",
    "            searchURL = self.config.searchURL.format(searchTerm, self.config.maxAge)\n",
    "            logger.debug(f\"\\tgetJobs:\\t {searchURL}\")\n",
    "            page = requests.get(searchURL, headers=self.config.headers, proxies=self.config.proxies)\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "            cards = soup.select(self.config.jobCardSelector)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"\\tgetJobs:\\t unable to retrieve jobs from {searchURL}\\n{e}\")\n",
    "            return jobs\n",
    "        \n",
    "        # get all the jobs\n",
    "        for card in cards:\n",
    "            try:\n",
    "                name = card.select_one(self.config.nameSelector).text.strip()\n",
    "                fuzzy = fuzz.ratio(self.config.searchTerm, name)\n",
    "                href = self.config.baseURL + card.select_one(self.config.hrefSelector)['href']\n",
    "                postedRelative = card.select_one(self.config.dateSelector).text.strip().upper()\n",
    "                company = card.select_one(self.config.companySelector).text.strip()\n",
    "\n",
    "                # skip it if the fuzz score is too low\n",
    "                if (fuzzy < self.config.minFuzzScore):\n",
    "                    logger.info(f\"\\tgetJobs:\\tignoring [{fuzzy}]\\t{name}\")\n",
    "                    continue\n",
    "\n",
    "                logger.debug(f\"\\tgetJobs:\\ttrying to parse {postedRelative}\")\n",
    "                daysOld = 9999\n",
    "                try:\n",
    "                    if (postedRelative in ['TODAY', 'JUST POSTED', 'POSTED TODAY', '24H']):\n",
    "                        daysOld = 0\n",
    "                    else:\n",
    "                        i = re.findall(r\"^\\d+\", postedRelative)\n",
    "                        daysOld = int(i[0])\n",
    "                except:\n",
    "                    logger.error(f\"\\tgetJobs:\\tunable to parse {postedRelative}\")\n",
    "\n",
    "                # skip it if it's older than the max age\n",
    "                if (daysOld > self.config.maxAge):\n",
    "                    logger.info(f\"\\tgetJobs:\\tignoring {daysOld} days old - {name}\")\n",
    "                    continue\n",
    "\n",
    "                j = Job(name = name, \n",
    "                        url = href, \n",
    "                        description = None, \n",
    "                        company = company, \n",
    "                        location = None,\n",
    "                        posted = date.today() - timedelta(days=daysOld), \n",
    "                        closes = date.today() - timedelta(days=daysOld) + timedelta(days=14), \n",
    "                        stage = \"new\", \n",
    "                        fuzzyWuzzy = fuzzy, \n",
    "                        config = self.config, \n",
    "                        daysOld = daysOld)\n",
    "                \n",
    "                j.retrieveDetails()\n",
    "                jobs.append(j)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.info(f\"\\tgetJobs:\\tException: {e}\")\n",
    "            \n",
    "        return jobs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a configuration for each website you want to search\n",
    "configs = []\n",
    "\n",
    "# indeed\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic indeed config\",\n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.indeed.com/jobs?q={0}&l=remote&fromage={1}\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = defRegex,\n",
    "    baseURL = \"https://www.indeed.com\",\n",
    "    headers = None,\n",
    "    proxies = None,\n",
    "    \n",
    "    jobCardSelector = \".result\",\n",
    "    nameSelector = \".jobtitle\",\n",
    "    hrefSelector = \".jobtitle\",\n",
    "    dateSelector = \".date\",\n",
    "    companySelector = \"span.company\",\n",
    "    \n",
    "    locationSelector = \".icl-u-xs-mt--xs > div > div:nth-of-type(3)\",\n",
    "    detailSelector = \"#jobDescriptionText\"\n",
    "))\n",
    "\n",
    "# career builder\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic career builder config\", \n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.careerbuilder.com/jobs?posted={1}&pay=&cat1=&radius=&emp=&cb_apply=false&keywords={0}&location=&cb_workhome=true\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = defRegex,\n",
    "    baseURL = \"https://www.careerbuilder.com\",\n",
    "    headers = None,\n",
    "    proxies = None,\n",
    "    \n",
    "    jobCardSelector = \".data-results-content-parent\",\n",
    "    nameSelector = \".data-results-title\",\n",
    "    hrefSelector = \"a.job-listing-item\",\n",
    "    dateSelector = \".data-results-publish-time\",\n",
    "    companySelector = \".data-details > span:nth-of-type(1)\",\n",
    "    \n",
    "    locationSelector = \".data-details > span:nth-of-type(3)\",\n",
    "    detailSelector = \"#jdp_description > div:nth-of-type(1) > div:nth-of-type(1)\"\n",
    "))\n",
    "\n",
    "# monster\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic monster config\",\n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.monster.com/jobs/search/?q={0}&tm={1}\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = defRegex,\n",
    "    baseURL = \"\",\n",
    "    headers = None,\n",
    "    proxies = None,\n",
    "    \n",
    "    jobCardSelector = \"div.flex-row\",\n",
    "    nameSelector = \".title\",\n",
    "    hrefSelector = \".title > a\",\n",
    "    dateSelector = \"time\",\n",
    "    companySelector = \"div.company\",\n",
    "    \n",
    "    locationSelector = \"div.location\",\n",
    "    detailSelector = \"div.job-description\"\n",
    "))\n",
    "\n",
    "# glass door\n",
    "configs.append(ScraperConfig(\n",
    "    searchTerm = \"generic glass door config\",\n",
    "    maxAge = defMaxAge,\n",
    "    searchURL = \"https://www.glassdoor.com/Job/{0}-jobs-SRCH_KO0,24.htm?fromAge={1}&remoteWorkType=1\",\n",
    "    \n",
    "    minFuzzScore = defMinFuzzScore,\n",
    "    regex = (\" \", \"-\"),\n",
    "    baseURL = \"https://www.glassdoor.com\",\n",
    "    headers = None,\n",
    "    proxies = None,\n",
    "    \n",
    "    jobCardSelector = \"li.jl\",\n",
    "    nameSelector = \"a.jobTitle > span\",\n",
    "    hrefSelector = \"a.jobTitle\",\n",
    "    dateSelector = \"div[data-test='job-age']\",\n",
    "    companySelector = \"a.jobLink > span\",\n",
    "    \n",
    "    locationSelector = \"div.flex-column > div > div:nth-of-type(3)\",\n",
    "    detailSelector = \"div.desc\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary for the specified port\n",
    "def getProxies(port=None):\n",
    "    if (port == None):\n",
    "        return None\n",
    "    \n",
    "    return { 'http': f'socks5://127.0.0.1:{port}',\n",
    "            'https': f'socks5://127.0.0.1:{port}' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually do the job scraping now!\n",
    "def main():\n",
    "    allJobs = []\n",
    "    scrapers = []\n",
    "    \n",
    "    # while there are still things to search for\n",
    "    while len(searchTerms) > 0:\n",
    "        logger.debug(f\"\\tmain:\\touter: {len(searchTerms)} terms remaining\")\n",
    "        \n",
    "        # create a thread pool for each search, site, and proxy combo\n",
    "        # Insert tasks into the queue and let them run\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            \n",
    "            # for each of the available ports\n",
    "            for port in torPorts:\n",
    "                \n",
    "                # and each of the terms\n",
    "                if len(searchTerms) > 0:\n",
    "                    search = searchTerms.pop()\n",
    "                    \n",
    "                    # for each of the configs\n",
    "                    for c in copy.deepcopy(configs):\n",
    "                        logger.debug(f\"\\tmain:\\tinner:\\tbase {c.baseURL}, search {search}, port {port}\")\n",
    "                        # fill in the config and create the scraper\n",
    "                        c.searchTerm = search\n",
    "                        c.proxies = getProxies(port)\n",
    "                        c.headers = {'User-Agent':UserAgent().random}\n",
    "                        js = JobScraper(c)\n",
    "                        \n",
    "                        # create a thread to fill in the job info\n",
    "                        futures.append(executor.submit(js.getJobs))\n",
    "                        \n",
    "            # if any of the ports are going to get reused,\n",
    "            if len(searchTerms) > 0:\n",
    "                # then get new IPs for each port\n",
    "                    with Controller.from_port(port = 9051) as c:\n",
    "                        c.authenticate()\n",
    "                        c.signal(Signal.NEWNYM)\n",
    "                        for port in torPorts:\n",
    "                            newIP = requests.get(\"https://ident.me\", proxies=getProxies(port)).text\n",
    "                            logger.info(f\"\\tgetJobs:\\tnew IP: {newIP}\")\n",
    "\n",
    "            # as threads complete, add their jobs to the allJobs list\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                logger.info(f\"\\tmain:\\tadding {len(future.result())} jobs to allJobs\")\n",
    "                \n",
    "                for r in future.result():\n",
    "                    logger.debug(f\"\\tmain:\\tadding {r.name} to allJobs\")\n",
    "                    allJobs.append(r)\n",
    "                    \n",
    "    return allJobs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveJobs(allJobs):\n",
    "    newFile = not(os.path.isfile(outfile) and os.path.getsize(outfile) > 0)\n",
    "    \n",
    "    # create our csv file for dataloader\n",
    "    with open(outfile, mode='a') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        if newFile:\n",
    "            writer.writerow(['Name','OriginalURL','Description','PostingDate','CloseDate','Stage'])\n",
    "\n",
    "        # then write each collection of jobs to the file\n",
    "        for j in allJobs:\n",
    "            writer.writerow(j.getDataloader())\n",
    "\n",
    "    logger.info(f\"\\tmain:\\t{outfile} created\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually do the job scraping now!\n",
    "if __name__ == \"__main__\":\n",
    "    starttime = datetime.now()\n",
    "    allJobs = main()\n",
    "    saveJobs(allJobs)\n",
    "    endtime = datetime.now()\n",
    "    logger.info(f\"\\ttime to run: {endtime - starttime}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
