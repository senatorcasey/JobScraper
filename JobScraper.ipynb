{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a job object\n",
    "class Job(object):\n",
    "    name = \"\"\n",
    "    url = \"\"\n",
    "    description = \"\"\n",
    "    company = \"\"\n",
    "    location = \"\"\n",
    "    posted = \"\"\n",
    "    closes = \"\"\n",
    "    stage = \"New\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "        \n",
    "    # mostly just for quick debugging\n",
    "    def __str__(self):\n",
    "        return f'Name: {self.name}\\n URL: {self.url}'\n",
    "    \n",
    "    def getDataloader(self):\n",
    "        return [self.name,\n",
    "               self.url,\n",
    "               self.description,\n",
    "               self.posted,\n",
    "               self.closes,\n",
    "               self.stage]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subclass for indeed jobs\n",
    "class IndeedJob(Job):\n",
    "    def retrieveDetails(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            div = soup.find('div', attrs={'class':'jobsearch-JobComponent-description'})\n",
    "            self.description = html2text.html2text(div.prettify())\n",
    "        except Exception as e:\n",
    "            print(\"Indeedjob.retriveDetails exception:\")\n",
    "            print(e)\n",
    "            print(self)\n",
    "            \n",
    "# create a subclass for indeed jobs\n",
    "class CareerBuilderJob(Job):\n",
    "    def retrieveDetails(self):\n",
    "        try:\n",
    "            # then follow the link for the rest of the details\n",
    "            response = requests.get(self.url)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            desc = soup.find('div',  attrs={'id':'jdp_description'})\n",
    "            self.description = html2text.html2text(desc.div.div.prettify())\n",
    "            \n",
    "            # the company and location are burried in unidentified spans\n",
    "            details = soup.find('div', class_=\"data-details\")\n",
    "            spans = details.find_all('span')\n",
    "            self.company = spans[0].text\n",
    "            self.location = spans[1].text\n",
    "        except Exception as e:\n",
    "            print(\"CareerBuilderJob.retriveDetails exception:\")\n",
    "            print(e)\n",
    "            print(self)\n",
    "            \n",
    "# create a subclass for indeed jobs\n",
    "class MonsterJob(Job):\n",
    "    def retrieveDetails(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            div = soup.find('div', attrs={'name':'sanitizedHtml'})\n",
    "            self.description = html2text.html2text(div.prettify())\n",
    "        except Exception as e:\n",
    "            print(\"MonsterJob.retriveDetails exception:\")\n",
    "            print(e)\n",
    "            print(self)\n",
    "\n",
    "            \n",
    "# create a subclass for indeed jobs\n",
    "class GlassDoorJob(Job):\n",
    "    def retrieveDetails(self):\n",
    "        try:\n",
    "            hdrs = {'user-agent': 'Mozilla/5.0'}\n",
    "            reponse = requests.get(self.url, headers=hdrs)\n",
    "            soup = BeautifulSoup(reponse.text, \"html.parser\")\n",
    "            div = soup.find('div', attrs={'id':'JobDescriptionContainer'})\n",
    "            self.description = html2text.html2text(div.prettify())\n",
    "        except Exception as e:\n",
    "            print(\"GlassDoorJob.retriveDetails exception:\")\n",
    "            print(e)\n",
    "            print(self)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from indeed\n",
    "def getIndeedJobs(results, what, maxAge):\n",
    "    request = f\"https://www.indeed.com/jobs?q={what}&l=remote&fromage={maxAge}\"\n",
    "    baseURL = \"https://www.indeed.com\"\n",
    "    \n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    # for every job card\n",
    "    for card in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        # find the job details\n",
    "        a = card.find('a', attrs={'data-tn-element':'jobTitle'})\n",
    "        c = card.find('span', attrs={'class':'company'})\n",
    "        l = card.find('span', attrs={'class':'location'})\n",
    "        d = card.find('span', attrs={'class':'date'})\n",
    "        \n",
    "        # figure out the dates\n",
    "        age = 0\n",
    "        try:\n",
    "            ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "            age = int(ds[0])\n",
    "        except:\n",
    "            if (d.text.strip().upper() == \"JUST POSTED\" or d.text.strip().upper() == \"TODAY\"):\n",
    "                pass\n",
    "            else:\n",
    "                continue # just ignore any jobs more than 30 days old\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # create a job object\n",
    "        j = IndeedJob(a.text.strip())\n",
    "        j.url = f\"{baseURL}{a['href']}\"\n",
    "        j.company = c.text.strip()\n",
    "        j.location = l.text.strip()\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "        # get the rest of the details in another thread\n",
    "        # just add the job object to our array of jobs\n",
    "        results.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from career builder\n",
    "def getCareerBuilderJobs(results, what, maxAge):\n",
    "    request = f\"https://www.careerbuilder.com/jobs?posted={maxAge}&pay=&cat1=&radius=&emp=&cb_apply=false&keywords={what}&location=&cb_workhome=true\"\n",
    "    baseURL = \"https://www.careerbuilder.com\"\n",
    "    \n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    for card in soup.find_all('div', class_=\"data-results-content-parent\"):\n",
    "        # find the particulars\n",
    "        n = card.find('div', class_=\"data-results-title\")\n",
    "        a = card.find('a', class_=\"data-results-content\")\n",
    "        d = card.find('div', class_=\"data-results-publish-time\")\n",
    "        \n",
    "        age = 0\n",
    "        try:\n",
    "            ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "            age = int(ds[0])\n",
    "        except:\n",
    "            if (d.text.strip().upper() == \"TODAY\"):\n",
    "                pass\n",
    "            else:\n",
    "                continue # who knows how old it is?!\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # career builder includes a lot of jobs regardless of age, so ignore those\n",
    "        if (age > maxAge):\n",
    "            continue\n",
    "        \n",
    "        # create the job\n",
    "        j = CareerBuilderJob(n.text.strip())\n",
    "        j.url = f\"{baseURL}{a['href']}\"\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "        # get the rest of the details in another thread\n",
    "        # just add the job object to our array of jobs\n",
    "        results.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from monster\n",
    "def getMonsterJobs(results, what, maxAge):\n",
    "    request = f\"https://www.monster.com/jobs/search/?q={what}&tm={maxAge}\"\n",
    "    baseURL = \"https://www.monster.com\"\n",
    "    \n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    for card in soup.find_all('div', class_=\"flex-row\"):\n",
    "        # extract the card details\n",
    "        n = card.h2\n",
    "        a = card.h2.a\n",
    "        c = card.find('div', class_=\"company\")\n",
    "        l = card.find('div', class_=\"location\")\n",
    "        d = card.find('time')\n",
    "        \n",
    "        age = 0\n",
    "        try:\n",
    "            ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "            age = int(ds[0])\n",
    "        except:\n",
    "            if (d.text.strip().upper() == \"POSTED TODAY\"):\n",
    "                pass\n",
    "            else:\n",
    "                continue # who knows how old it is?!\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # monster returns a lot of garbage,\n",
    "        # use fuzzywuzzy to ignore anything that isn't reasonably close\n",
    "        f = fuzz.ratio(what, n.text.strip())\n",
    "        if (f < 50): \n",
    "            #print(f\"{f}: ignoring {n.text.strip()}\")\n",
    "            continue\n",
    "        \n",
    "        # create the job\n",
    "        j = MonsterJob(n.text.strip())\n",
    "        j.url = a['href']\n",
    "        j.company = c.text.strip()\n",
    "        j.location = l.text.strip()\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "        # get the rest of the details in another thread\n",
    "        # just add the job object to our array of jobs\n",
    "        results.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from glass door\n",
    "def getGlassDoorJobs(results, what, maxAge):\n",
    "    what = re.sub(\" \", \"-\", what)\n",
    "    request = f\"https://www.glassdoor.com/Job/{what}-jobs-SRCH_KO0,24.htm?fromAge={maxAge}&remoteWorkType=1\"\n",
    "    baseURL = \"https://www.glassdoor.com\"\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    for card in soup.find_all('li', class_=\"react-job-listing\"):\n",
    "        # extract the card details\n",
    "        n = card.find('a', class_=\"jobTitle\")\n",
    "        c = card.find('div', class_=\"jobHeader\")\n",
    "        l = card.find('span', class_='loc')\n",
    "        d = card.find('div', attrs={'data-test':'job-age'})\n",
    "        \n",
    "        age = 0\n",
    "        if (d.text.strip() != \"24h\"):\n",
    "            try:\n",
    "                ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "                age = int(ds[0])\n",
    "            except:\n",
    "                continue # who knows how old it is?!\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # glass door returns a lot of garbage,\n",
    "        # use fuzzywuzzy to ignore anything that isn't reasonably close\n",
    "        f = fuzz.ratio(what, n.text.strip())\n",
    "        if (f < 60):\n",
    "            #print(f\"{f}: ignoring {n.text.strip()}\")\n",
    "            continue\n",
    "        \n",
    "        # create the job\n",
    "        j = GlassDoorJob(n.text)\n",
    "        j.company = c.text\n",
    "        j.location = l.text\n",
    "        j.url = f\"{baseURL}{n['href']}\"\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "        # get the rest of the details in another thread\n",
    "        # just add the job object to our array of jobs\n",
    "        results.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting jobs\n",
      "found 23 jobs, retrieving details\n",
      "pool has 23 elements\n",
      "MonsterJob.retriveDetails exception:\n",
      "'NoneType' object has no attribute 'prettify'\n",
      "Name: Salesforce Administrator\n",
      " URL: https://job-openings.monster.com/salesforce-administrator-denver-co-us-k12-inc/de37e6ce-da5e-4f76-ba6a-1da4bb7d7647\n",
      "MonsterJob.retriveDetails exception:\n",
      "'NoneType' object has no attribute 'prettify'\n",
      "Name: Salesforce Administrator\n",
      " URL: https://job-openings.monster.com/salesforce-administrator-colorado-springs-co-us-cherwell-software/c6717e08-0358-4f33-8151-79ca9d6fe285\n",
      "time to run: 0:00:04.298315\n",
      "job count: 23\n",
      "threading.csv created\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    starttime = datetime.now()\n",
    "    allJobs = []\n",
    "    \n",
    "    print(\"getting jobs\")\n",
    "    \n",
    "    # Create a pool with four worker threads\n",
    "    pool = []\n",
    "\n",
    "    # Insert tasks into the queue and let them run\n",
    "    pool.append(threading.Thread(target=getIndeedJobs, args=(allJobs, \"salesforce administrator\", 1)))\n",
    "    pool.append(threading.Thread(target=getCareerBuilderJobs, args=(allJobs, \"salesforce administrator\", 1)))\n",
    "    pool.append(threading.Thread(target=getMonsterJobs, args=(allJobs, \"salesforce administrator\", 1)))\n",
    "    pool.append(threading.Thread(target=getGlassDoorJobs, args=(allJobs, \"salesforce administrator\", 1)))\n",
    "    \n",
    "    # start all the threads\n",
    "    for thread in pool:\n",
    "        thread.start()\n",
    "    \n",
    "    # wait for the threads\n",
    "    for thread in pool:\n",
    "        thread.join()\n",
    "    \n",
    "    print(f\"found {len(allJobs)} jobs, retrieving details\")\n",
    "    # create a new pool to retrieve the details of every job\n",
    "    pool = []\n",
    "    for j in allJobs:\n",
    "        pool.append(threading.Thread(target=j.retrieveDetails))\n",
    "    \n",
    "    print(f\"pool has {len(pool)} elements\")\n",
    "    # start all the threads\n",
    "    for thread in pool:\n",
    "        thread.start()\n",
    "    \n",
    "    # wait for the threads\n",
    "    for thread in pool:\n",
    "        thread.join()\n",
    "        \n",
    "    endtime = datetime.now()\n",
    "    print(f\"time to run: {endtime - starttime}\")\n",
    "    print(f\"job count: {len(allJobs)}\")\n",
    "    \n",
    "    #print(*allJobs, sep='\\n')\n",
    "    \n",
    "    # create our csv file for dataloader\n",
    "    outfile = 'threading.csv'\n",
    "    with open(outfile, mode='w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(['Name','OriginalURL','Description','PostingDate','CloseDate','Stage'])\n",
    "\n",
    "        # then write each collection of jobs to the file\n",
    "\n",
    "        for j in allJobs:\n",
    "            writer.writerow(j.getDataloader())\n",
    "\n",
    "    print(f\"{outfile} created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
