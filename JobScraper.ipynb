{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import re\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a job object\n",
    "class Job:\n",
    "    name = \"\"\n",
    "    url = \"\"\n",
    "    description = \"\"\n",
    "    company = \"\"\n",
    "    location = \"\"\n",
    "    stage = \"New\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # default the posting and closing dates to now + 7\n",
    "        dt = date.today()\n",
    "        self.posted = dt.strftime(\"%m/%d/%Y\")\n",
    "        dt += timedelta(days=7)\n",
    "        self.closes = dt.strftime(\"%m/%d/%Y\")\n",
    "        \n",
    "    # mostly just for quick debugging\n",
    "    def __str__(self):\n",
    "        return f'Name: {self.name}, Posted: {self.posted}, Closes: {self.closes}'\n",
    "    \n",
    "    def getDataloader(self):\n",
    "        return [self.name,\n",
    "               self.url,\n",
    "               self.description,\n",
    "               self.posted,\n",
    "               self.closes,\n",
    "               self.stage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from indeed\n",
    "def getIndeedJobs(what, where=\"remote\", maxAge=1):\n",
    "    request = f\"https://www.indeed.com/jobs?q={what}&l={where}&fromage={maxAge}\"\n",
    "    baseURL = \"https://www.indeed.com\"\n",
    "    jobs = []\n",
    "    \n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    # for every job card\n",
    "    for card in soup.find_all('div', attrs={'data-tn-component':'organicJob'}):\n",
    "        # find the job details\n",
    "        a = card.find('a', attrs={'data-tn-element':'jobTitle'})\n",
    "        c = card.find('span', attrs={'class':'company'})\n",
    "        l = card.find('span', attrs={'class':'location'})\n",
    "        d = card.find('span', attrs={'class':'date'})\n",
    "        \n",
    "        # figure out the dates\n",
    "        age = 0\n",
    "        try:\n",
    "            ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "            age = int(ds[0])\n",
    "        except:\n",
    "            if (d.text.strip().upper() == \"JUST POSTED\" or d.text.strip().upper() == \"TODAY\"):\n",
    "                pass\n",
    "            else:\n",
    "                continue # just ignore any jobs more than 30 days old\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # create a job object\n",
    "        j = Job(a.text.strip())\n",
    "        j.url = f\"{baseURL}{a['href']}\"\n",
    "        j.company = c.text.strip()\n",
    "        j.location = l.text.strip()\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        # the full description is on another page,\n",
    "        # so follow the URL to that!\n",
    "        p2 = requests.get(j.url)\n",
    "        s2 = BeautifulSoup(p2.text, \"html.parser\")\n",
    "        d = s2.find('div', attrs={'class':'jobsearch-JobComponent-description'})\n",
    "        j.description = html2text.html2text(d.prettify())\n",
    "\n",
    "        # finally add the job object to our array of jobs\n",
    "        jobs.append(j)\n",
    "        \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from career builder\n",
    "def getCareerBuilderJobs(what, where=\"\", maxAge=1):\n",
    "    request = f\"https://www.careerbuilder.com/jobs?posted={maxAge}&pay=&cat1=&radius=&emp=&cb_apply=false&keywords={what}&location={where}&cb_workhome=true\"\n",
    "    baseURL = \"https://www.careerbuilder.com\"\n",
    "    jobs = []\n",
    "\n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    for card in soup.find_all('div', class_=\"data-results-content-parent\"):\n",
    "        # find the particulars\n",
    "        n = card.find('div', class_=\"data-results-title\")\n",
    "        a = card.find('a', class_=\"data-results-content\")\n",
    "        d = card.find('div', class_=\"data-results-publish-time\")\n",
    "        \n",
    "        age = 0\n",
    "        try:\n",
    "            ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "            age = int(ds[0])\n",
    "        except:\n",
    "            if (d.text.strip().upper() == \"TODAY\"):\n",
    "                pass\n",
    "            else:\n",
    "                continue # who knows how old it is?!\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # career builder includes a lot of jobs regardless of age, so ignore those\n",
    "        if (age > maxAge):\n",
    "            continue\n",
    "        \n",
    "        # create the job\n",
    "        j = Job(n.text.strip())\n",
    "        j.url = f\"{baseURL}{a['href']}\"\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "        \n",
    "        # then follow the link for the rest of the details\n",
    "        p2 = requests.get(j.url)\n",
    "        s2 = BeautifulSoup(p2.text, \"html.parser\")\n",
    "        d = s2.find('div',  attrs={'id':'jdp_description'})\n",
    "        j.description = html2text.html2text(d.div.div.prettify())\n",
    "\n",
    "        # the company and location are burried in unidentified spans\n",
    "        x = s2.find('div', class_=\"data-details\")\n",
    "        x2 = x.find_all('span')\n",
    "        j.company = x2[0].text\n",
    "        j.location = x2[1].text\n",
    "\n",
    "        # finally add the job object to our array of jobs\n",
    "        jobs.append(j)\n",
    "    \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from monster\n",
    "def getMonsterJobs(what, maxAge=1):\n",
    "    request = f\"https://www.monster.com/jobs/search/?q={what}&tm={maxAge}\"\n",
    "    baseURL = \"https://www.monster.com\"\n",
    "    jobs = []\n",
    "\n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    for card in soup.find_all('div', class_=\"flex-row\"):\n",
    "        # extract the card details\n",
    "        n = card.h2\n",
    "        a = card.h2.a\n",
    "        c = card.find('div', class_=\"company\")\n",
    "        l = card.find('div', class_=\"location\")\n",
    "        d = card.find('time')\n",
    "        \n",
    "        age = 0\n",
    "        try:\n",
    "            ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "            age = int(ds[0])\n",
    "        except:\n",
    "            if (d.text.strip().upper() == \"POSTED TODAY\"):\n",
    "                pass\n",
    "            else:\n",
    "                continue # who knows how old it is?!\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # monster returns a lot of garbage,\n",
    "        # use fuzzywuzzy to ignore anything that isn't reasonably close\n",
    "        f = fuzz.ratio(what, n.text.strip())\n",
    "        if (f < 40): \n",
    "            continue\n",
    "            \n",
    "        # create the job\n",
    "        j = Job(n.text.strip())\n",
    "        j.url = a['href']\n",
    "        j.company = c.text.strip()\n",
    "        j.location = l.text.strip()\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        # then follow the link for the full description\n",
    "        try:\n",
    "            p2 = requests.get(j.url)\n",
    "            s2 = BeautifulSoup(p2.text, \"html.parser\")\n",
    "            d = s2.find('div', attrs={'name':'sanitizedHtml'})\n",
    "            j.description = html2text.html2text(d.prettify())\n",
    "        except:\n",
    "            print(f\"unable to get description from {j.url}\")\n",
    "            \n",
    "        # finally add the job object to our array of jobs\n",
    "        jobs.append(j)\n",
    "        \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the jobs from glass door\n",
    "def getGlassDoorJobs(what, maxAge=1):\n",
    "    what = re.sub(\" \", \"-\", what)\n",
    "    request = f\"https://www.glassdoor.com/Job/{what}-jobs-SRCH_KO0,24.htm?fromAge={maxAge}&remoteWorkType=1\"\n",
    "    baseURL = \"https://www.glassdoor.com\"\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    jobs = []\n",
    "\n",
    "    # create the request and soup objects\n",
    "    page = requests.get(request, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    for card in soup.find_all('li', class_=\"react-job-listing\"):\n",
    "        # extract the card details\n",
    "        n = card.find('a', class_=\"jobTitle\")\n",
    "        c = card.find('div', class_=\"jobHeader\")\n",
    "        l = card.find('span', class_='loc')\n",
    "        d = card.find('div', attrs={'data-test':'job-age'})\n",
    "        \n",
    "        age = 0\n",
    "        if (d.text.strip() != \"24h\"):\n",
    "            try:\n",
    "                ds = re.findall(r\"^\\d+\", d.text.strip())\n",
    "                age = int(ds[0])\n",
    "            except:\n",
    "                continue # who knows how old it is?!\n",
    "        posted = date.today() - timedelta(days=age)\n",
    "        closes = posted + timedelta(weeks=4)\n",
    "        \n",
    "        # glass door returns a lot of garbage,\n",
    "        # use fuzzywuzzy to ignore anything that isn't reasonably close\n",
    "        f = fuzz.ratio(what, n.text.strip())\n",
    "        if (f < 60):\n",
    "            continue\n",
    "        \n",
    "        # create the job\n",
    "        j = Job(n.text)\n",
    "        j.company = c.text\n",
    "        j.location = l.text\n",
    "        j.url = f\"{baseURL}{n['href']}\"\n",
    "        j.posted = posted.strftime(\"%m/%d/%Y\")\n",
    "        j.closes = closes.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "        # then follow the link for the full description\n",
    "        p2 = requests.get(j.url, headers=headers)\n",
    "        s2 = BeautifulSoup(p2.text, \"html.parser\")\n",
    "        d = s2.find('div', attrs={'id':'JobDescriptionContainer'})\n",
    "        j.description = html2text.html2text(d.prettify())\n",
    "\n",
    "        # finally add the job object to our array of jobs\n",
    "        jobs.append(j)\n",
    "        \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching indeed...\n",
      "job count: 10\n",
      "fetching career builder...\n",
      "job count: 12\n",
      "fetching monster...\n",
      "unable to get description from https://job-openings.monster.com/development-communications-assistant-boston-ma-us-randstad/221458657\n",
      "unable to get description from https://job-openings.monster.com/salesforce-administrator-denver-co-us-k12-inc/de37e6ce-da5e-4f76-ba6a-1da4bb7d7647\n",
      "unable to get description from https://job-openings.monster.com/salesforce-administrator-colorado-springs-co-us-cherwell-software/c6717e08-0358-4f33-8151-79ca9d6fe285\n",
      "job count: 30\n",
      "fetching glass door...\n",
      "job count: 34\n",
      "running time: 0:00:23.181201\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "\n",
    "# create a master list and add each site to that\n",
    "allJobs = []\n",
    "print(\"fetching indeed...\")\n",
    "allJobs.extend(getIndeedJobs(\"salesforce administrator\"))\n",
    "print(f\"job count: {len(allJobs)}\")\n",
    "\n",
    "print(\"fetching career builder...\")\n",
    "allJobs.extend(getCareerBuilderJobs(\"salesforce administrator\"))\n",
    "print(f\"job count: {len(allJobs)}\")\n",
    "\n",
    "print(\"fetching monster...\")\n",
    "allJobs.extend(getMonsterJobs(\"salesforce administrator\"))\n",
    "print(f\"job count: {len(allJobs)}\")\n",
    "\n",
    "print(\"fetching glass door...\")\n",
    "allJobs.extend(getGlassDoorJobs(\"salesforce administrator\"))\n",
    "print(f\"job count: {len(allJobs)}\")\n",
    "\n",
    "endtime = datetime.now()\n",
    "print(f\"running time: {endtime-starttime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Part-Time Salesforce Administrator Consultant (Remote Positi..., Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: SalesForce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Sales Support Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Salesforce Developer - Cognisight/GRIPA, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Service Contract Administrator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Sr Sales Executive | Healthcare, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: COMMERCIAL LOAN CLOSING ADMINISTRATOR, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Sales Representative - K-12 Education Technology (Southeast/..., Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator - U.S. Telecommute, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Field Systems Analyst, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator / Developer, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Business Process Analyst- SalesForce Systems Administrator, JIRA, ZenDesk, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: SalesForce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: IT Architect II (Salesforce Developer & Administrator), Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Salesforce Administrator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Administrative Assistant, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Developer (Einstein Analytics), Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Administrative Assistant / Project Coordinator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Program Coordinator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Salesforce Developer - SalesForce/Lightning/Certified Developer, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Administrative Assistant / Project Coordinator, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Development/Communications Assistant, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Salesforce Developer - with Salesforce Platform Developer cert, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Developer - REMOTE - East Coast time zone, Posted: 11/24/2020, Closes: 12/22/2020\n",
      "Name: Tax Administrative Assistant, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: IT Systems Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Senior Salesforce Administrator, Posted: 11/25/2020, Closes: 12/23/2020\n",
      "Name: Salesforce Administrator & Business Analyst, Posted: 11/25/2020, Closes: 12/23/2020\n"
     ]
    }
   ],
   "source": [
    "print(*allJobs, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraper.csv created\n",
      "0:46:58.440508\n"
     ]
    }
   ],
   "source": [
    "# create our csv file for dataloader\n",
    "outfile = 'scraper.csv'\n",
    "with open(outfile, mode='w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['Name','OriginalURL','Description','PostingDate','CloseDate','Stage'])\n",
    "    \n",
    "    # then write each collection of jobs to the file\n",
    "    \n",
    "    for j in allJobs:\n",
    "        writer.writerow(j.getDataloader())\n",
    "        \n",
    "print(f\"{outfile} created\")\n",
    "endtime = datetime.now()\n",
    "print(endtime - starttime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
